{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"Download data to local machine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load configuration.py\n",
    "configuration = {\n",
    "    \"pg_url\": \"jdbc:postgresql://localhost:5432/honeypot\",\n",
    "    \"pg_host\": \"localhost\",\n",
    "    \"pg_port\": \"5432\",\n",
    "    \"pg_database\": \"honeypot\",\n",
    "    \"pg_user\": \"max\",\n",
    "    \"pg_password\": \"kM9ZhBOBFIl\",\n",
    "    \"spark_host\": \"10.10.10.80\",\n",
    "    \"spark_uri\": \"spark://10.10.10.80:7077\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data.py\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "dates = [\n",
    "    \"2021-05-17\", #  0: first monday to start logging on\n",
    "    \"2021-05-18\", #  1:\n",
    "    \"2021-05-19\", #  2:\n",
    "    \"2021-05-20\", #  3:\n",
    "    \"2021-05-21\", #  4\n",
    "    \"2021-05-22\", #  5: Saturday\n",
    "    \"2021-05-23\", #  6: Sunday\n",
    "    \"2021-05-24\", #  7: Monday (Whit monday)\n",
    "    \"2021-05-25\", #  8:\n",
    "    \"2021-05-26\", #  9:\n",
    "    \"2021-05-27\", # 10: \n",
    "    \"2021-05-28\", # 11:\n",
    "    \"2021-05-29\", # 12: Saturday\n",
    "    \"2021-05-30\", # 13: Sunday\n",
    "    \"2021-05-31\", # 14: first dayout logging\n",
    "]\n",
    "\n",
    "dates = [parser.parse(d) for d in dates]\n",
    "\n",
    "# Get the date filters per week day.\n",
    "# Consider that each filter consists of a tuple (lower & upper bound)\n",
    "\n",
    "def get_i_weekday(day: int):\n",
    "    \"\"\"Returns a list of indices which represent the corresponding week day.\"\"\"\n",
    "    b = day % 7\n",
    "    return [b, b + 7]\n",
    "\n",
    "def get_name_weekday(day: int) -> str:\n",
    "    \"\"\"Returns the name of the weekday.\"\"\"\n",
    "    if day == 0:\n",
    "        return \"Monday\"\n",
    "    elif day == 1:\n",
    "        return \"Tuesday\"\n",
    "    elif day == 2:\n",
    "        return \"Wednesday\"\n",
    "    elif day == 3:\n",
    "        return \"Thursday\"\n",
    "    elif day == 4:\n",
    "        return \"Friday\"\n",
    "    elif day == 5:\n",
    "        return \"Saturday\"\n",
    "    elif day == 6:\n",
    "        return \"Sunday\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid day of the week {day}\")\n",
    "    \n",
    "    \n",
    "# honeypots\n",
    "\n",
    "honeypots = [\n",
    "    ('campus', 1),\n",
    "    ('campus', 2),\n",
    "    ('campus', 3),\n",
    "    ('residential', 6),\n",
    "    ('residential', 7),\n",
    "    ('residential', 8),\n",
    "    ('residential', 9),\n",
    "    ('cloud', 12),\n",
    "    ('cloud', 13),\n",
    "    ('cloud', 14),\n",
    "    ('cloud', 15),\n",
    "]\n",
    "\n",
    "# categories\n",
    "categories = ['ssh', 'telnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'log' into dataframe\n",
      "Storing 'log' dataframe in CSV /home/max/jupyter/dataplane/raw/log.csv\n",
      "Successfully stored CSV\n",
      "Loading 'dataplane' into dataframe\n",
      "Storing 'dataplane' dataframe in CSV /home/max/jupyter/dataplane/raw/dataplane.csv\n",
      "Successfully stored CSV\n",
      "Loading 'ip' into dataframe\n",
      "Storing 'log' dataframe in CSV /home/max/jupyter/dataplane/raw/ip.csv\n",
      "Successfully stored CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "dbc = psycopg2.connect(\n",
    "    database=configuration['pg_database'],\n",
    "    host=configuration['pg_host'],\n",
    "    port=configuration['pg_port'],\n",
    "    user=configuration['pg_user'], \n",
    "    password=configuration['pg_password'],\n",
    ")\n",
    "\n",
    "print(\"Loading 'log' into dataframe\")\n",
    "df_log = pd.read_sql_query(\n",
    "    f\"SELECT * FROM log where '{dates[0]}' <= timestamp AND timestamp < '{dates[-1]}'\",\n",
    "    con=dbc,\n",
    "    index_col=['id'],\n",
    ")\n",
    "\n",
    "print(f\"Storing 'log' dataframe in CSV {csv_log}\")\n",
    "df_log.to_csv(csv_log)\n",
    "print(\"Successfully stored CSV\")\n",
    "\n",
    "print(\"Loading 'dataplane' into dataframe\")\n",
    "df_dataplane = pd.read_sql_query(\n",
    "    f\"SELECT * FROM dataplane where '{dates[0]}' <= timestamp AND timestamp < '{dates[-1]}'\",\n",
    "    con=dbc,\n",
    "    index_col=['id']\n",
    ")\n",
    "\n",
    "print(f\"Storing 'dataplane' dataframe in CSV {csv_dataplane}\")\n",
    "df_dataplane.to_csv(csv_dataplane)\n",
    "print(\"Successfully stored CSV\")\n",
    "\n",
    "\n",
    "print(\"Loading 'ip' into dataframe\")\n",
    "df_ip = pd.read_sql_query(\n",
    "    f\"SELECT * FROM ip;\",\n",
    "    con=dbc,\n",
    "    index_col=['id'],\n",
    ")\n",
    "\n",
    "print(f\"Storing 'log' dataframe in CSV {csv_ip}\")\n",
    "df_ip.to_csv(csv_ip)\n",
    "print(\"Successfully stored CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of dataframes\n",
    "\n",
    "The following section first generates the filter on which the log data needs\n",
    "to be filtered. Afterwards, the dataframe is created. The data frames are\n",
    "filtered per honeypot, per distinct IP addresses and number of requests in\n",
    "total. Also, the data is represented cummulative and as a partial filter per\n",
    "slot. It is only data shown based on SSH requests and on Telnet requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sync_ts</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>asn</th>\n",
       "      <th>asname</th>\n",
       "      <th>category</th>\n",
       "      <th>ip</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-19 23:45:01+00:00</td>\n",
       "      <td>2021-05-19 22:21:31+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>MIT-GATEWAYS</td>\n",
       "      <td>ssh</td>\n",
       "      <td>18.27.197.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-19 23:45:01+00:00</td>\n",
       "      <td>2021-05-19 22:59:11+00:00</td>\n",
       "      <td>137</td>\n",
       "      <td>ASGARR Consortium GARR</td>\n",
       "      <td>ssh</td>\n",
       "      <td>131.114.98.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-05-19 23:45:01+00:00</td>\n",
       "      <td>2021-05-19 04:25:38+00:00</td>\n",
       "      <td>174</td>\n",
       "      <td>COGENT-174</td>\n",
       "      <td>ssh</td>\n",
       "      <td>185.142.236.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-05-19 23:45:01+00:00</td>\n",
       "      <td>2021-05-19 12:17:34+00:00</td>\n",
       "      <td>174</td>\n",
       "      <td>COGENT-174</td>\n",
       "      <td>ssh</td>\n",
       "      <td>185.142.236.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-05-19 23:45:01+00:00</td>\n",
       "      <td>2021-05-19 01:08:44+00:00</td>\n",
       "      <td>174</td>\n",
       "      <td>COGENT-174</td>\n",
       "      <td>ssh</td>\n",
       "      <td>185.142.236.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sync_ts                 timestamp  asn  \\\n",
       "id                                                            \n",
       "1  2021-05-19 23:45:01+00:00 2021-05-19 22:21:31+00:00    3   \n",
       "2  2021-05-19 23:45:01+00:00 2021-05-19 22:59:11+00:00  137   \n",
       "5  2021-05-19 23:45:01+00:00 2021-05-19 04:25:38+00:00  174   \n",
       "6  2021-05-19 23:45:01+00:00 2021-05-19 12:17:34+00:00  174   \n",
       "7  2021-05-19 23:45:01+00:00 2021-05-19 01:08:44+00:00  174   \n",
       "\n",
       "                    asname category              ip  \n",
       "id                                                   \n",
       "1             MIT-GATEWAYS      ssh   18.27.197.252  \n",
       "2   ASGARR Consortium GARR      ssh   131.114.98.64  \n",
       "5               COGENT-174      ssh  185.142.236.34  \n",
       "6               COGENT-174      ssh  185.142.236.35  \n",
       "7               COGENT-174      ssh  185.142.236.36  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log.head()\n",
    "\n",
    "df_dataplane.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Generate all Filters\\n# ================\\n\\n# First define the time deltas (in minutes) which you would like to display\\n# at the end of this notebook.\\n\\n#deltas = [3, 5, 10, 15, 30, 60]\\ndeltas = [60, 120]\\n\\n# The remainin code generates the filters based on the deltas given above.\\n\\nfrom datetime import timedelta\\n\\n# Time delta in Minutes:\\n\\ndeltas = [timedelta(minutes=d) for d in deltas]\\nlog_start = dates[1]\\nlog_end = dates[5]\\n\\n# filters partially for each partition\\nfilters_par = {}\\n# filters cumulative from beginning to end for each parition\\nfilters_cum = {}\\n\\n# format\\nfmt = '%Y-%m-%d %H:%M:%S'\\n\\nfor d in deltas:\\n    filter_par = []\\n    filter_cum = []\\n    \\n    x = log_start + d\\n    filter_par.append(\\n        (format(log_start, fmt), format(x, fmt))\\n    )\\n    \\n    while x < log_end:\\n        filter_par.append(\\n            (format(x, fmt), format(x + d, fmt))\\n        )\\n        filter_cum.append(\\n            (format(log_start, fmt), format(x, fmt))\\n        )\\n        \\n        x += d\\n    \\n    filters_par[d] = filter_par\\n    filters_cum[d] = filter_cum\\n    \\n    \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Generate all Filters\n",
    "# ================\n",
    "\n",
    "# First define the time deltas (in minutes) which you would like to display\n",
    "# at the end of this notebook.\n",
    "\n",
    "#deltas = [3, 5, 10, 15, 30, 60]\n",
    "deltas = [60, 120]\n",
    "\n",
    "# The remainin code generates the filters based on the deltas given above.\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Time delta in Minutes:\n",
    "\n",
    "deltas = [timedelta(minutes=d) for d in deltas]\n",
    "log_start = dates[1]\n",
    "log_end = dates[5]\n",
    "\n",
    "# filters partially for each partition\n",
    "filters_par = {}\n",
    "# filters cumulative from beginning to end for each parition\n",
    "filters_cum = {}\n",
    "\n",
    "# format\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "for d in deltas:\n",
    "    filter_par = []\n",
    "    filter_cum = []\n",
    "    \n",
    "    x = log_start + d\n",
    "    filter_par.append(\n",
    "        (format(log_start, fmt), format(x, fmt))\n",
    "    )\n",
    "    \n",
    "    while x < log_end:\n",
    "        filter_par.append(\n",
    "            (format(x, fmt), format(x + d, fmt))\n",
    "        )\n",
    "        filter_cum.append(\n",
    "            (format(log_start, fmt), format(x, fmt))\n",
    "        )\n",
    "        \n",
    "        x += d\n",
    "    \n",
    "    filters_par[d] = filter_par\n",
    "    filters_cum[d] = filter_cum\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_log.dtypes\\n\\n\\n#pdf = df_log.toPandas()\\n\\n#keys = []\\n#dfs = {}\\n\\n#for h in honeypots:\\n#    keys += [(cat, h[0], h[1]) for cat in categories]\\n    \\n\\n#for k in keys:\\n#    df = df_log.select(l\\n#        \\'id\\',\\n#        \\'category\\',\\n#        \\'origin\\',\\n#        \\'origin_id\\',\\n#        \\'timestamp\\',\\n#        \\'ip\\'\\n#    ).filter(\\n#        f\"category == \\'{k[0]}\\'\"\\n#    ).filter(\\n#        f\"origin == \\'{k[1]}\\'\"\\n#    ).filter(\\n#        f\"origin_id == \\'{k[2]}\\'\"\\n#    ).filter(\\n#        f\"\\'{format(log_start, fmt)}\\' <= timestamp and timestamp < \\'{format(log_end, fmt)}\\'\"\\n#    )\\n#        \\n#    dfs[k] = df\\n#\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dataframes per honeypot and category\n",
    "# ================\n",
    "# Load the dataframe to a Pandas DF\n",
    "# Since the DB is considerably small (< 1 GB) , we hope that the use of\n",
    "# pandas will speed up the data anlysis.\n",
    "\"\"\"\n",
    "df_log.dtypes\n",
    "\n",
    "\n",
    "#pdf = df_log.toPandas()\n",
    "\n",
    "#keys = []\n",
    "#dfs = {}\n",
    "\n",
    "#for h in honeypots:\n",
    "#    keys += [(cat, h[0], h[1]) for cat in categories]\n",
    "    \n",
    "\n",
    "#for k in keys:\n",
    "#    df = df_log.select(l\n",
    "#        'id',\n",
    "#        'category',\n",
    "#        'origin',\n",
    "#        'origin_id',\n",
    "#        'timestamp',\n",
    "#        'ip'\n",
    "#    ).filter(\n",
    "#        f\"category == '{k[0]}'\"\n",
    "#    ).filter(\n",
    "#        f\"origin == '{k[1]}'\"\n",
    "#    ).filter(\n",
    "#        f\"origin_id == '{k[2]}'\"\n",
    "#    ).filter(\n",
    "#        f\"'{format(log_start, fmt)}' <= timestamp and timestamp < '{format(log_end, fmt)}'\"\n",
    "#    )\n",
    "#        \n",
    "#    dfs[k] = df\n",
    "#\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TODO: Rewrite this part...\\n# Dataframe cummulative & partial\\n# The outermost container of the dict is the timedelta.\\n# In the time delta is the key \"category, network type, honeypot\" wrapped.\\n# The innermost layer contains the values per timeslot (depending on the time delta)\\ndf_req_par = {}\\ndf_req_cum = {}\\n\\nfor d in deltas:\\n    for f in filters_par[d]:       \\n        df = df_log.select(\\n            \\'category\\',\\'origin\\', \\'origin_id\\', \\'id\\'\\n        ).filter(\\n            f\"\\'{f[0]}\\' <= timestamp AND timestamp < \\'{f[1]}\\'\"\\n        ).groupBy(\\'category\\', \\'origin\\', \\'origin_id\\').count().withColumn(\\n            \"timestamp\", fn.lit(f[0])\\n        ).select(\\n            \\'timestamp\\', \\'category\\', \\'origin\\', \\'origin_id\\', \\'count\\'\\n        ).orderBy(\\n            \\'timestamp\\', \\'category\\', \\'origin\\', \\'origin_id\\'\\n        )\\n        \\n        if not df_req_par.get(d):\\n            df_req_par[d] = df\\n        else:\\n            df_req_par[d] = df_req_par[d].union(df)\\n        \\n    print(f\\'Finished partial evaluation on time delta {str(d)}\\')\\n\\n    for f in filters_cum[d]:\\n        df = df_log.select(\\n            \\'category\\', \\'origin\\', \\'origin_id\\', \\'id\\'\\n        ).filter(\\n            f\"\\'{f[0]}\\' <= timestamp AND timestamp < \\'{f[1]}\\'\"\\n        ).groupBy(\\'category\\', \\'origin\\', \\'origin_id\\').count().withColumn(\\n            \\'timestamp\\', fn.lit(f[0])\\n        ).select(\\n            \\'timestamp\\', \\'category\\', \\'origin\\', \\'origin_id\\', \\'count\\'\\n        ).orderBy(\\n            \\'timestamp\\', \\'category\\', \\'origin\\', \\'origin_id\\'\\n        )\\n        \\n        if not df_req_cum.get(d):\\n            df_req_cum[d] = df\\n        else:\\n            df_req_cum[d] = df_req_cum[d].union(df)\\n        \\n    print(f\\'Finished cumulative evaluation on time delta {str(d)}\\')\\n\\n    \\nfor d in deltas:\\n    print(f\"df_req_par[{d}]: # {df_req_par[d].count()}\")\\n    print(f\"df_req_cum[{d}]: # {df_req_cum[d].count()}\")\\n    \\n    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "# TODO: Rewrite this part...\n",
    "# Dataframe cummulative & partial\n",
    "# The outermost container of the dict is the timedelta.\n",
    "# In the time delta is the key \"category, network type, honeypot\" wrapped.\n",
    "# The innermost layer contains the values per timeslot (depending on the time delta)\n",
    "df_req_par = {}\n",
    "df_req_cum = {}\n",
    "\n",
    "for d in deltas:\n",
    "    for f in filters_par[d]:       \n",
    "        df = df_log.select(\n",
    "            'category','origin', 'origin_id', 'id'\n",
    "        ).filter(\n",
    "            f\"'{f[0]}' <= timestamp AND timestamp < '{f[1]}'\"\n",
    "        ).groupBy('category', 'origin', 'origin_id').count().withColumn(\n",
    "            \"timestamp\", fn.lit(f[0])\n",
    "        ).select(\n",
    "            'timestamp', 'category', 'origin', 'origin_id', 'count'\n",
    "        ).orderBy(\n",
    "            'timestamp', 'category', 'origin', 'origin_id'\n",
    "        )\n",
    "        \n",
    "        if not df_req_par.get(d):\n",
    "            df_req_par[d] = df\n",
    "        else:\n",
    "            df_req_par[d] = df_req_par[d].union(df)\n",
    "        \n",
    "    print(f'Finished partial evaluation on time delta {str(d)}')\n",
    "\n",
    "    for f in filters_cum[d]:\n",
    "        df = df_log.select(\n",
    "            'category', 'origin', 'origin_id', 'id'\n",
    "        ).filter(\n",
    "            f\"'{f[0]}' <= timestamp AND timestamp < '{f[1]}'\"\n",
    "        ).groupBy('category', 'origin', 'origin_id').count().withColumn(\n",
    "            'timestamp', fn.lit(f[0])\n",
    "        ).select(\n",
    "            'timestamp', 'category', 'origin', 'origin_id', 'count'\n",
    "        ).orderBy(\n",
    "            'timestamp', 'category', 'origin', 'origin_id'\n",
    "        )\n",
    "        \n",
    "        if not df_req_cum.get(d):\n",
    "            df_req_cum[d] = df\n",
    "        else:\n",
    "            df_req_cum[d] = df_req_cum[d].union(df)\n",
    "        \n",
    "    print(f'Finished cumulative evaluation on time delta {str(d)}')\n",
    "\n",
    "    \n",
    "for d in deltas:\n",
    "    print(f\"df_req_par[{d}]: # {df_req_par[d].count()}\")\n",
    "    print(f\"df_req_cum[{d}]: # {df_req_cum[d].count()}\")\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npdf_req_par = {}\\npdf_req_cum = {}\\n\\nfor d in deltas:\\n    pdf_req_par[d] = df_req_par[d].toPandas()\\n    pdf_req_cum[d] = df_req_cum[d].toPandas()\\n    \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pdf_req_par = {}\n",
    "pdf_req_cum = {}\n",
    "\n",
    "for d in deltas:\n",
    "    pdf_req_par[d] = df_req_par[d].toPandas()\n",
    "    pdf_req_cum[d] = df_req_cum[d].toPandas()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
