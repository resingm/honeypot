{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"Dataplane Ramp-up Analysis per Honeypot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load configuration.py\n",
    "configuration = {\n",
    "    \"pg_url\": \"jdbc:postgresql://localhost:5432/honeypot\",\n",
    "    \"pg_user\": \"max\",\n",
    "    \"pg_password\": \"kM9ZhBOBFIl\",\n",
    "    \"spark_host\": \"10.10.10.80\",\n",
    "    \"spark_uri\": \"spark://10.10.10.80:7077\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load setup.py\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark');\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(configuration['spark_uri']) \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config('spark.driver.host', configuration['spark_host']) \\\n",
    "    .config('spark.jars', 'postgresql-42.2.20.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "options = {\n",
    "    \"url\": configuration['pg_url'],\n",
    "    \"user\": configuration['pg_user'],\n",
    "    \"password\": configuration[\"pg_password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "df_log = spark.read.format(\"jdbc\").options(\n",
    "    dbtable=\"log\",\n",
    "    **options,\n",
    ").load()\n",
    "\n",
    "df_dp = spark.read.format(\"jdbc\").options(\n",
    "    dbtable=\"dataplane\",\n",
    "    **options,\n",
    ").load()\n",
    "\n",
    "df_ip = spark.read.format(\"jdbc\").options(\n",
    "    dbtable=\"ip\",\n",
    "    **options,\n",
    ").load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data.py\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "dates = [\n",
    "    \"2021-05-17\", #  0: first monday to start logging on\n",
    "    \"2021-05-18\", #  1:\n",
    "    \"2021-05-19\", #  2:\n",
    "    \"2021-05-20\", #  3:\n",
    "    \"2021-05-21\", #  4\n",
    "    \"2021-05-22\", #  5: Saturday\n",
    "    \"2021-05-23\", #  6: Sunday\n",
    "    \"2021-05-24\", #  7: Monday (Whit monday)\n",
    "    \"2021-05-25\", #  8:\n",
    "    \"2021-05-26\", #  9:\n",
    "    \"2021-05-27\", # 10: \n",
    "    \"2021-05-28\", # 11:\n",
    "    \"2021-05-29\", # 12: Saturday\n",
    "    \"2021-05-30\", # 13: Sunday\n",
    "    \"2021-05-31\", # 14: first dayout logging\n",
    "]\n",
    "\n",
    "dates = [parser.parse(d) for d in dates]\n",
    "\n",
    "# Get the date filters per week day.\n",
    "# Consider that each filter consists of a tuple (lower & upper bound)\n",
    "\n",
    "def get_i_weekday(day: int):\n",
    "    \"\"\"Returns a list of indices which represent the corresponding week day.\"\"\"\n",
    "    b = day % 7\n",
    "    return [b, b + 7]\n",
    "\n",
    "def get_name_weekday(day: int) -> str:\n",
    "    \"\"\"Returns the name of the weekday.\"\"\"\n",
    "    if day == 0:\n",
    "        return \"Monday\"\n",
    "    elif day == 1:\n",
    "        return \"Tuesday\"\n",
    "    elif day == 2:\n",
    "        return \"Wednesday\"\n",
    "    elif day == 3:\n",
    "        return \"Thursday\"\n",
    "    elif day == 4:\n",
    "        return \"Friday\"\n",
    "    elif day == 5:\n",
    "        return \"Saturday\"\n",
    "    elif day == 6:\n",
    "        return \"Sunday\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid day of the week {day}\")\n",
    "    \n",
    "    \n",
    "# honeypots\n",
    "\n",
    "honeypots = [\n",
    "    ('campus', 1),\n",
    "    ('campus', 2),\n",
    "    ('campus', 3),\n",
    "    ('residential', 6),\n",
    "    ('residential', 7),\n",
    "    ('residential', 8),\n",
    "    ('residential', 9),\n",
    "    ('cloud', 12),\n",
    "    ('cloud', 13),\n",
    "    ('cloud', 14),\n",
    "    ('cloud', 15),\n",
    "]\n",
    "\n",
    "# categories\n",
    "categories = ['ssh', 'telnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of dataframes\n",
    "\n",
    "The following section first generates the filter on which the log data needs\n",
    "to be filtered. Afterwards, the dataframe is created. The data frames are\n",
    "filtered per honeypot, per distinct IP addresses and number of requests in\n",
    "total. Also, the data is represented cummulative and as a partial filter per\n",
    "slot. It is only data shown based on SSH requests and on Telnet requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all Filters\n",
    "# ================\n",
    "\n",
    "# First define the time deltas (in minutes) which you would like to display\n",
    "# at the end of this notebook.\n",
    "\n",
    "#deltas = [3, 5, 10, 15, 30, 60]\n",
    "deltas = [60, 120]\n",
    "\n",
    "# The remainin code generates the filters based on the deltas given above.\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Time delta in Minutes:\n",
    "\n",
    "deltas = [timedelta(minutes=d) for d in deltas]\n",
    "log_start = dates[1]\n",
    "log_end = dates[5]\n",
    "\n",
    "# filters partially for each partition\n",
    "filters_par = {}\n",
    "# filters cumulative from beginning to end for each parition\n",
    "filters_cum = {}\n",
    "\n",
    "# format\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "for d in deltas:\n",
    "    filter_par = []\n",
    "    filter_cum = []\n",
    "    \n",
    "    x = log_start + d\n",
    "    filter_par.append(\n",
    "        (format(log_start, fmt), format(x, fmt))\n",
    "    )\n",
    "    \n",
    "    while x < log_end:\n",
    "        filter_par.append(\n",
    "            (format(x, fmt), format(x + d, fmt))\n",
    "        )\n",
    "        filter_cum.append(\n",
    "            (format(log_start, fmt), format(x, fmt))\n",
    "        )\n",
    "        \n",
    "        x += d\n",
    "    \n",
    "    filters_par[d] = filter_par\n",
    "    filters_cum[d] = filter_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframes per honeypot and category\n",
    "# ================\n",
    "# This will speedup the selection process of the next steps.\n",
    "\n",
    "keys = []\n",
    "dfs = {}\n",
    "\n",
    "for h in honeypots:\n",
    "    keys += [(cat, h[0], h[1]) for cat in categories]\n",
    "    \n",
    "for k in keys:\n",
    "    df = df_log.select(\n",
    "        'id',\n",
    "        'category',\n",
    "        'origin',\n",
    "        'origin_id',\n",
    "        'timestamp',\n",
    "        'ip'\n",
    "    ).filter(\n",
    "        f\"category == '{k[0]}'\"\n",
    "    ).filter(\n",
    "        f\"origin == '{k[1]}'\"\n",
    "    ).filter(\n",
    "        f\"origin_id == '{k[2]}'\"\n",
    "    ).filter(\n",
    "        f\"'{format(log_start, fmt)}' <= timestamp and timestamp < '{format(log_end, fmt)}'\"\n",
    "    )\n",
    "        \n",
    "    dfs[k] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished partial evaluation on time delta 1:00:00\n",
      "Finished cumulative evaluation on time delta 1:00:00\n",
      "Finished partial evaluation on time delta 2:00:00\n",
      "Finished cumulative evaluation on time delta 2:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ec2a6077052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"df_req_par[{d}]: # {df_req_par[d].count()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"df_req_cum[{d}]: # {df_req_cum[d].count()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Rewrite this part...\n",
    "# Dataframe cummulative & partial\n",
    "# The outermost container of the dict is the timedelta.\n",
    "# In the time delta is the key \"category, network type, honeypot\" wrapped.\n",
    "# The innermost layer contains the values per timeslot (depending on the time delta)\n",
    "df_req_par = {}\n",
    "df_req_cum = {}\n",
    "\n",
    "for d in deltas:\n",
    "    for f in filters_par[d]:       \n",
    "        df = df_log.select(\n",
    "            'category','origin', 'origin_id', 'id'\n",
    "        ).filter(\n",
    "            f\"'{f[0]}' <= timestamp AND timestamp < '{f[1]}'\"\n",
    "        ).groupBy('category', 'origin', 'origin_id').count().withColumn(\n",
    "            \"timestamp\", fn.lit(f[0])\n",
    "        ).select(\n",
    "            'timestamp', 'category', 'origin', 'origin_id', 'count'\n",
    "        ).orderBy(\n",
    "            'timestamp', 'category', 'origin', 'origin_id'\n",
    "        )\n",
    "        \n",
    "        if not df_req_par.get(d):\n",
    "            df_req_par[d] = df\n",
    "        else:\n",
    "            df_req_par[d] = df_req_par[d].union(df)\n",
    "        \n",
    "    print(f'Finished partial evaluation on time delta {str(d)}')\n",
    "\n",
    "    for f in filters_cum[d]:\n",
    "        df = df_log.select(\n",
    "            'category', 'origin', 'origin_id', 'id'\n",
    "        ).filter(\n",
    "            f\"'{f[0]}' <= timestamp AND timestamp < '{f[1]}'\"\n",
    "        ).groupBy('category', 'origin', 'origin_id').count().withColumn(\n",
    "            'timestamp', fn.lit(f[0])\n",
    "        ).select(\n",
    "            'timestamp', 'category', 'origin', 'origin_id', 'count'\n",
    "        ).orderBy(\n",
    "            'timestamp', 'category', 'origin', 'origin_id'\n",
    "        )\n",
    "        \n",
    "        if not df_req_cum.get(d):\n",
    "            df_req_cum[d] = df\n",
    "        else:\n",
    "            df_req_cum[d] = df_req_cum[d].union(df)\n",
    "        \n",
    "    print(f'Finished cumulative evaluation on time delta {str(d)}')\n",
    "\n",
    "    \n",
    "for d in deltas:\n",
    "    print(f\"df_req_par[{d}]: # {df_req_par[d].count()}\")\n",
    "    print(f\"df_req_cum[{d}]: # {df_req_cum[d].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_req_par = {}\n",
    "pdf_req_cum = {}\n",
    "\n",
    "for d in deltas:\n",
    "    pdf_req_par[d] = df_req_par[d].toPandas()\n",
    "    pdf_req_cum[d] = df_req_cum[d].toPandas()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
